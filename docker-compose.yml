version: '3.8'

services:
  # Open WebUI with bundled Ollama (GPU enabled)
  open-webui:
    image: ghcr.io/open-webui/open-webui:ollama
    container_name: ai-open-webui-ollama
    ports:
      - "3000:8080"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_BASE_URL=http://localhost:11434
      - WEBUI_SECRET_KEY=${SECRET_KEY:-your-secret-key-change-this}
      - WEBUI_AUTH=true
      - DEFAULT_USER_ROLE=user
      - ENABLE_SIGNUP=true
      - ENABLE_LOGIN_FORM=true
      # Auto-download DeepSeek model on startup
      - OLLAMA_MODELS=deepseek-coder-v2:16b
    volumes:
      - ollama_data:/root/.ollama
      - open_webui_data:/app/backend/data
      # Mount initialization script to auto-download model
      - ./scripts/init-models.sh:/app/init-models.sh
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: ${OLLAMA_MEMORY_LIMIT:-20G}
        reservations:
          cpus: '2.0'
          memory: ${OLLAMA_MEMORY_RESERVATION:-12G}
    runtime: nvidia
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # NGINX Reverse Proxy
  nginx:
    build: ./nginx
    container_name: ai-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/static:/usr/share/nginx/html/static:ro
      - nginx_cache:/var/cache/nginx
    depends_on:
      - open-webui
    networks:
      - ai-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped

networks:
  ai-network:
    driver: bridge

volumes:
  ollama_data:
  open_webui_data:
  nginx_cache: