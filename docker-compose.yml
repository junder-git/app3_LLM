version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    build: ./db
    container_name: deepseek-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-deepseek}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - deepseek-network

  # Web UI Application
  web-ui:
    build: ./web-ui
    container_name: deepseek-web-ui
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-deepseek}
      POSTGRES_HOST: postgres
      OLLAMA_HOST: ollama
      OLLAMA_PORT: 11434
      MODEL_NAME: deepseek-coder:33b-instruct-q5_K_M
      SESSION_SECRET: ${SESSION_SECRET:-changeme_in_production}
      ADMIN_USERNAME: ${ADMIN_USERNAME:-admin}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD:-admin}
      GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    restart: unless-stopped
    networks:
      - deepseek-network

  # Nginx Reverse Proxy
  nginx:
    build: ./nginx
    container_name: deepseek-nginx
    depends_on:
      - web-ui
    ports:
      - "${PORT:-80}:80"
    volumes:
      - ./nginx/static:/usr/share/nginx/html/static
    restart: unless-stopped
    networks:
      - deepseek-network

  # Ollama Model Server with GPU acceleration
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    container_name: deepseek-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    # Memory and CPU allocation
    deploy:
      resources:
        limits:
          memory: 24G  # Allocate 24GB of memory (adjust based on your system)
        reservations:
          memory: 16G  # Guarantee at least 16GB
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_NUM_GPU_LAYERS=50  # Optional: Set number of GPU layers if needed
      - OLLAMA_RAM_SIZE=16GB      # Optional: Set RAM usage limit
    restart: unless-stopped
    networks:
      - deepseek-network

networks:
  deepseek-network:
    driver: bridge

volumes:
  postgres_data:
  ollama_data: